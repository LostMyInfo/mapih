// @ts-check

/**
 * @typedef {Object} AnthropicMessage
 * @property {string} id - Unique object identifier
 * @property {AnthropicContentBlock[]} content - Content generated by the model
 * @property {string} model - The model that handled the request
 * @property {string} role - Will always be 'assistant'
 * @property {?string} stop_reason - The reason that we stopped
 * @property {?string} stop_sequence - Which custom stop sequence was generated, if any
 * @property {string} type - Will always be 'message'
 * @property {AnthropicUsage} usage - Billing and rate-limit usage
 */

/**
 * @typedef {Object} AnthropicContentBlock
 * @property {string} text
 * @property {string} type - Always 'text'
 */

/**
 * @typedef {Object} AnthropicTextBlock
 * @property {string} text
 * @property {string} [type] - Always 'text'
 */

/**
 * @typedef {Object} AnthropicImageBlockParam
 * @property {AnthropicImageBlockParamSource} source
 * @property {string} [type] - Always 'image'
 */

/**
 * @typedef {Object} AnthropicImageBlockParamSource
 * @property {string} data
 * @property {string} media_type - 'image/jpeg', 'image/png', 'image/gif', 'image/webp'
 * @property {string} [type] - Always 'base64'
 */

/**
 * @typedef {Object} AnthropicUsage
 * @property {number} input_tokens - The number of input tokens which were used
 * @property {number} output_tokens - The number of output tokens which were used
 */

/**
 * @typedef {Object} AnthropicMessageParam
 * @property {string | Array<AnthropicTextBlock | AnthropicImageBlockParam>} content
 * @property {string} role - 'user' or 'assistant'
 */

/**
 * @typedef {Object} AnthropicMessageCreateParams
 * @property {string} model - The model that will complete your prompt
 * @property {number} max_tokens - The maximum number of tokens to generate before stopping
 * @property {AnthropicMessageParam[]} messages - Input messages
 * @property {{user_id: ?string}} [metadata] - An object describing metadata about the request
 * @property {string[]} [stop_sequence] - Custom text sequences that will cause the model to stop generating
 * @property {boolean} [stream] - Whether to incrementally stream the response using server-sent events
 * @property {string} [system] - System prompt
 * @property {number} [temperature] - Amount of randomness injected into the response. Defaults to `1.0`.
 * @property {number} [top_k] - Only sample from the top K options for each subsequent token
 * @property {number} [top_p] - Use nucleus sampling
 */

